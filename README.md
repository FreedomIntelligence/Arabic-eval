# Arabic-eval
Datasets and description containing whole process of Arabic model evaluation task.<br>
****
## Datasets<br>
### Raw data
There are 3 datasets in folder ``data`` used to generate response.<br>
#### ``vicuna80.json``
The format of this file is suitable for other models except GPT4 model which in folder GPT-API-Accerlerate .<br>
We first translated querys from English to Arabic.Then we asked a native Arabic speaker to check if English and Arabic matched and rewrite the results in Arabic if it is not good.
#### ``vicuna80_toGPT.jsonl``
The format of this file is suitable for  GPT4  model which in folder GPT-API-Accerlerate.<br>
#### ``vicuna80_GPT_Result.jsonl``
This file contains results for instructions in vicuna_toGPT generated by GPT4.<br>
### Generated data
There are many datasets in folder ``gen_data`` generated by differents models.You can use them to evaluate the performance of different models.<br>
****
## How to run GPT4?
You can go to this page to learn  functions  : https://github.com/FreedomIntelligence/GPT4-API-Accelerate<br>
And if you want to run GPT4, you need to edit and run  ``demo_main.py``.  
****
## How to run other models?
You can go to this page to learn:https://github.com/FreedomIntelligence/LLM-eval-pipeline
To run this model,you can run it on machine at ``bloom-eval/LLM-eval-pipeline-main``,then start a ``terminal``,and Run the following code:  
python generate.py \
    --model_id ${model_id=llama-7b-hf} \
    --generation_type ${generation_type=greedy} \
    --data_name ${data_name=vicuna80} \
    --batch_size ${batch_size=1}
You have to change some information if you need to try other models and datasets.
    
## How to evaluate the performance of different models?



